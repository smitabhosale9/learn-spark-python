{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "def random_subset(X, y, dims, n_shuffle=10, seed=42):\n",
    "  \"\"\"Selects a random subset of X and y according to the dimensions\n",
    "  \n",
    "  Params:\n",
    "    X: n x d pandas dataframe\n",
    "    y: n x 1 pandas dataframe \n",
    "    dims: list of tuples\n",
    "    n_shuffle: run n_shuffle shuffle operations on the set of indices\n",
    "    seed: seed the random number generator\n",
    "  \n",
    "  Returns:\n",
    "    X', y': sampled dataframes\n",
    "  Example:\n",
    "    Select only 75% of the values with target 0, and all values\n",
    "    where target is 1\n",
    "    $ dims = [(0, 0.75), (1, 1.0)]\n",
    "  \"\"\"\n",
    "  np.random.seed(seed)\n",
    "  idx = []\n",
    "  \n",
    "  for target, factor in dims:\n",
    "    if (0 <= factor < 1.0):\n",
    "      n_samples = int(len(y[y == target]) * factor)\n",
    "      idx_sub = np.random.choice(y.index[y == target], n_samples, replace=False)\n",
    "    else:\n",
    "      idx_sub = y.index[y == target]\n",
    "    # Stack the indices together  \n",
    "    idx = np.hstack((idx, idx_sub))\n",
    "  for i in range(n_shuffle):\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "  return X.loc[idx.astype(int)], y[idx.astype(int)]\n",
    "\n",
    "\n",
    "def truncate(value, max_length=100, suffix=\"...\", pre=5):\n",
    "    if len(value) > max_length:\n",
    "      return value[0:pre] + suffix + value[pre+len(suffix)+1:max_length+1]\n",
    "    else:\n",
    "      return value\n",
    "\n",
    "def score(*args, **kwargs):\n",
    "  \"\"\"Decorator, that transform a function to a scorer.\n",
    "  A scorer has the arguments estimator, X, y_true, sample_weight=None\n",
    "  \"\"\"\n",
    "  decorator_args = args\n",
    "  decorator_kwargs = kwargs\n",
    "  def score_decorator(func):\n",
    "    @wraps(func)\n",
    "    def func_wrapper(*args, **kwargs):\n",
    "      func_args = args\n",
    "      func_kwargs = kwargs\n",
    "      scorer = make_scorer(func, *decorator_args, **decorator_kwargs)\n",
    "      return scorer(*func_args, **func_kwargs)\n",
    "    return func_wrapper\n",
    "  return score_decorator\n",
    "\n",
    "def folds(y, n_folds=4, stratified=False, random_state=42, shuffle=True, **kwargs):\n",
    "  if stratified:\n",
    "    return cv.StratifiedKFold(y, n_folds=n_folds, shuffle=shuffle, random_state=random_state, **kwargs)\n",
    "  return cv.KFold(n=len(y), n_folds=n_folds, shuffle=shuffle, random_state=random_state, **kwargs)\n",
    "\n",
    "def cross_val(estimator, X, y, n_jobs=-1, n_folds=4, proba=False, **kwargs):\n",
    "  # Extract values from pandas DF\n",
    "  if hasattr(X, 'values'):\n",
    "    X = X.values\n",
    "  if hasattr(y, 'values'):\n",
    "    y = y.values\n",
    "\n",
    "  # Return Cross validation score\n",
    "  if proba is True:\n",
    "    estimator.predict = lambda self, *args, **kwargs: self.predict_proba(*args, **kwargs)[:,1]\n",
    "\n",
    "  return cv.cross_val_score(estimator, X, y, cv=folds(y, n_folds=n_folds), n_jobs=n_jobs, **kwargs)\n",
    "\n",
    "\n",
    "class BaseTransform(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "  \"\"\"Transform Interface\"\"\"\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y=None, **fit_params):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    return X\n",
    "\n",
    "class PandasTransform(BaseTransform):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def transform(self, X):\n",
    "    return X.values\n",
    "\n",
    "\n",
    "class Log1pTransform(BaseTransform):\n",
    "  def __init__(self, columns=None):\n",
    "    self.columns = columns=None\n",
    "\n",
    "  def transform(self, X):\n",
    "    if self.columns:\n",
    "      for column in self.columns:\n",
    "        X[column] = np.log1p(X[column])\n",
    "        return X\n",
    "    else:\n",
    "      return np.log1p(X)\n",
    "\n",
    "  def inverse_transform(self, X):\n",
    "    if self.columns:\n",
    "      for column in self.columns:\n",
    "        X[column] = np.expm1(X[column])\n",
    "        return X\n",
    "    else:\n",
    "      return np.expm1(X)\n",
    "\n",
    "\n",
    "class NanPreProcessor(TransformerMixin):\n",
    "  \"\"\"Fills NaN with class median\n",
    "  @source: https://www.kaggle.com/cbrogan/titanic/xgboost-example-python/code\n",
    "  @based: http://stackoverflow.com/a/25562948\"\"\"\n",
    "  def fit(self, X, y=None):\n",
    "    self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "      if X[c].dtype == np.dtype('O') else X[c].median() for c in X], index=X.columns)\n",
    "    return self\n",
    "  def transform(self, X, y=None):\n",
    "    return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "def tsne_plot(X, y, title=\"\", metric='l1', random_state=0, legend_loc='upper left', n_samples=None, n_components=2):\n",
    "  \"\"\"Plots the first 2 components of the t-distributed Stochastic Neighbor Embedding\n",
    "  References:\n",
    "   * http://blog.kaggle.com/2012/11/02/t-distributed-stochastic-neighbor-embedding-wins-merck-viz-challenge/\"\"\"\n",
    "\n",
    "  if n_samples:\n",
    "      # Get the shape of the training set\n",
    "      n_samples_orig, n_features = np.shape(X)\n",
    "\n",
    "      # Select 5000 random indices\n",
    "      rnd_indices = np.random.choice(n_samples_orig, n_samples)\n",
    "\n",
    "      X = X[rnd_indices]\n",
    "      y = y[rnd_indices]\n",
    "\n",
    "  # Create a t-SNE model\n",
    "  model = TSNE(n_components=n_components, random_state=random_state, metric=metric)\n",
    "  X_trans = model.fit_transform(X)\n",
    "\n",
    "  # Get a list of unique labels\n",
    "  labels = np.unique(y)\n",
    "\n",
    "  # This is only needed to adjust the size of the figure\n",
    "  # because otherwise it is really small\n",
    "  plt.figure(figsize=(15, 15), dpi=120)\n",
    "\n",
    "  # Get a list of color values\n",
    "  colors = cm.rainbow(np.linspace(0, 1, len(labels) * 2))\n",
    "\n",
    "  # Loop over labels\n",
    "  # enumerate also return the index from the list\n",
    "  for i, label in enumerate(labels):\n",
    "\n",
    "      # Get a feature vector with the matching label\n",
    "      # and add a scatter plot with the dataset\n",
    "      plt.scatter(X_trans[y == label][:,0], X_trans[y == label][:,1], c=colors[i], label=label)\n",
    "\n",
    "  # Add a legend\n",
    "  plt.legend(loc=legend_loc)\n",
    "\n",
    "  # Add axis labels\n",
    "  plt.xlabel(\"1st component\")\n",
    "  plt.ylabel(\"2nd component\")\n",
    "\n",
    "  # Add a title\n",
    "  plt.title(title)\n",
    "\n",
    "  # Render the plot\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def duplicate_columns(data):\n",
    "    \"\"\"Find columns that are a duplicate of other columns\n",
    "  \n",
    "    Params:\n",
    "      data  pd.DataFrame\n",
    "  \n",
    "    Returns:\n",
    "      list of column labels\n",
    "    \"\"\"\n",
    "    correlation = data.corr()\n",
    "    \n",
    "    # Create a diagonal condition to filter the correlation of a column with itself\n",
    "    diag_mask = np.zeros(correlation.shape, dtype='bool')\n",
    "    np.fill_diagonal(diag_mask, True)\n",
    "\n",
    "    # Creates a mask of equal columns\n",
    "    equal_mask = np.isclose(correlation.mask(cond=diag_mask).abs().values, 1.0)\n",
    "    \n",
    "    original_columns = set()\n",
    "    duplicate_columns = set()\n",
    "    \n",
    "    # Iterate through the columns\n",
    "    for col in np.unique(correlation[equal_mask].index):\n",
    "        # Get all perfectly correlated cols\n",
    "        cols = list(correlation[col][np.isclose(correlation.ix[col].abs(), 1.0)].index)\n",
    "  \n",
    "        # Sort by length\n",
    "        cols.sort(key=len)\n",
    "\n",
    "        # Find the original col\n",
    "        for c in cols:\n",
    "            if c in original_columns:\n",
    "                original_col = c\n",
    "                break\n",
    "        else:\n",
    "            original_col = cols[0]\n",
    "            original_columns.add(original_col)\n",
    "            \n",
    "        # Remove the original column\n",
    "        cols.remove(original_col)\n",
    "        \n",
    "        # Add the column to the duplicate cols\n",
    "        for c in cols:\n",
    "            duplicate_columns.add(c)\n",
    "\n",
    "    return list(duplicate_columns)\n",
    "        \n",
    "def zero_var_columns(data):\n",
    "    \"\"\"Find columns containing zero variance data\n",
    "  \n",
    "    Params:\n",
    "      data  pd.DataFrame\n",
    "  \n",
    "    Returns:\n",
    "      list of column labels\n",
    "    \"\"\"\n",
    "    u = data.apply(lambda x: len(x.unique()))\n",
    "    return list(u[u == 1].index.values)\n",
    "\n",
    "class Table(object):\n",
    "  def __init__(self, max_col_width=30):\n",
    "    self.values = OrderedDict()\n",
    "    self.size = 0\n",
    "    self.max_col_width = max_col_width\n",
    "\n",
    "  def add_column(self, label, values):\n",
    "    if label in self.values:\n",
    "      raise ValueError('Duplicate Column')\n",
    "    self.values[label] = values\n",
    "    self.size = max(len(values), self.size)\n",
    "\n",
    "  def max_length(self, col):\n",
    "    return max(max(list(map(lambda c: len(str(c)), self.values[col]))), len(col))\n",
    "\n",
    "  def html(self):\n",
    "    output = \"\"\n",
    "\n",
    "    output += \"<table>\"\n",
    "\n",
    "    output += \"<thead>\"\n",
    "    output += \"<tr>\"\n",
    "    for col in self.values:\n",
    "      output +=  '<th>{name:s}</th>'.format(name=col)\n",
    "    output += \"</tr>\"\n",
    "    output += \"</thead>\"\n",
    "\n",
    "    output += \"<tbody>\"\n",
    "    for r in range(self.size):\n",
    "      output += \"<tr>\"\n",
    "      for col in self.values:\n",
    "        output += '<td>{name:s}</td>'.format(name=str(self.values[col][r]))\n",
    "      output += \"</tr>\"\n",
    "    output += \"</tbody>\"\n",
    "\n",
    "    output += \"</table>\"\n",
    "    return output\n",
    "\n",
    "  def __str__(self):\n",
    "    col_sep = \" |\"\n",
    "    output = \"\"\n",
    "\n",
    "    dim = {col: min(self.max_length(col), self.max_col_width) for col in self.values}\n",
    "\n",
    "    for col in self.values:\n",
    "      output +=  ' {name:{fill}<{width}s}'.format(name=truncate(col, dim[col]), fill=\" \", width=dim[col])\n",
    "      output += col_sep\n",
    "    output += \"\\n\"\n",
    "\n",
    "    for col in self.values:\n",
    "      output +=  ' {name:{fill}<{width}s}'.format(name=\"\", fill=\"-\", width=dim[col])\n",
    "      output += col_sep\n",
    "    output += \"\\n\"\n",
    "\n",
    "    for r in range(self.size):\n",
    "      for col in self.values:\n",
    "        output += ' {name:{fill}<{width}s}'.format(name=truncate(str(self.values[col][r]), dim[col]), fill=' ', width=dim[col])\n",
    "        output += col_sep\n",
    "      output += \"\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_categoric_columns(data):\n",
    "    return data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "def get_numeric_columns(data):\n",
    "    return data.select_dtypes(exclude=['object', 'category']).columns\n",
    "\n",
    "def pretty_stats(data, stat=None, target_key=None):\n",
    "  \"\"\"Generate a pretty statistic about the dataframe *data*\"\"\"\n",
    "\n",
    "  cat_columns = get_categoric_columns(data)\n",
    "  num_columns = get_numeric_columns(data)\n",
    "\n",
    "  if not stat or stat is 'general':\n",
    "    table = Table()\n",
    "\n",
    "    table.add_column('property', values=[\n",
    "      'Number of features',\n",
    "      'Number of categorical features',\n",
    "      'Number of numerical features',\n",
    "      'Number of Samples',\n",
    "    ])\n",
    "\n",
    "    table.add_column('values', values=[\n",
    "      len(data.columns),\n",
    "      len(cat_columns),\n",
    "      len(num_columns),\n",
    "      len(data),\n",
    "    ])\n",
    "\n",
    "    display(HTML('<h1>General</h1>'))\n",
    "    display(HTML(table.html()))\n",
    "\n",
    "  if target_key and (not stat or stat is 'target'):\n",
    "    table = Table()\n",
    "    aggregate = data.groupby([target_key]).agg({data.columns[0]:len})\n",
    "\n",
    "    table.add_column('target', values=aggregate.index.values)\n",
    "    table.add_column('count', values=aggregate.values.flatten())\n",
    "\n",
    "    display(HTML('<h1>Distribution per Target</h1>'))\n",
    "    display(HTML(table.html()))\n",
    "\n",
    "  if not stat or stat is 'distribution':\n",
    "    table = Table()\n",
    "    num_data = data[num_columns]\n",
    "    distribution = num_data.describe()\n",
    "    \n",
    "    table.add_column('feature', values=list(num_data.columns))\n",
    "    table.add_column('Unique', values=num_data.apply(lambda x: len(x.unique())))\n",
    "    table.add_column('NaN', values=num_data.isnull().sum().values)\n",
    "    table.add_column('min', values=distribution.ix['min'].values)\n",
    "    table.add_column('min count', values=num_data[num_data == num_data.min()].count())\n",
    "    table.add_column('mean', values=distribution.ix['mean'].values)\n",
    "    table.add_column('max', values=distribution.ix['max'].values)\n",
    "    table.add_column('max count', values=num_data[num_data == num_data.max()].count())\n",
    "\n",
    "    display(HTML('<h1>Distribution of Numerical Values</h1>'))\n",
    "    display(HTML(table.html()))\n",
    "    \n",
    "    table = Table()\n",
    "    cat_data = data[cat_columns]\n",
    "    \n",
    "    table.add_column('feature', values=list(cat_data.columns))\n",
    "    table.add_column('Num Categories', values=cat_data.apply(lambda x: len(x.unique())))\n",
    "    table.add_column('Categories', values=cat_data.apply(lambda x: list(set(x))))\n",
    "    table.add_column('NaN', values=cat_data.isnull().sum().values)\n",
    "    \n",
    "    display(HTML('<h1>Distribution of Categorical Features</h1>'))\n",
    "    display(HTML(table.html()))\n",
    "\n",
    "  if not stat or stat is 'correlation':\n",
    "    table = Table()\n",
    "    num_data = data[num_columns]\n",
    "    correlation = num_data.corr()\n",
    "\n",
    "    # Create a diagonal condition to filter the correlation of a column with itself\n",
    "    diag_mask = np.zeros(correlation.shape, dtype='bool')\n",
    "    np.fill_diagonal(diag_mask, True)\n",
    "\n",
    "    table.add_column('feature', values=list(num_data.columns))\n",
    "    table.add_column('highest value', values=correlation.mask(cond=diag_mask).abs().max(skipna=True).values)\n",
    "    table.add_column('correlated with', values=correlation.mask(cond=diag_mask).abs().idxmax(skipna=True).values)\n",
    "    table.add_column('mean', values=correlation.mask(cond=diag_mask).abs().mean().values)\n",
    "\n",
    "    display(HTML('<h1>Correlation of Numerical Features</h1>'))\n",
    "    display(HTML(table.html()))\n",
    "    \n",
    "def feature_importance(X, y, criterion='entropy', n_estimators=250, random_state=0):\n",
    "  clf = ExtraTreesClassifier(n_estimators=n_estimators, random_state=random_state, criterion=criterion)\n",
    "  clf = clf.fit(X, y)\n",
    "  importances = clf.feature_importances_\n",
    "  std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "  indices = np.argsort(importances)[::-1]\n",
    "\n",
    "  return pd.DataFrame({\"column\":X.columns, \"importance\":importances, \"std\": std}).set_index(indices)\n",
    "\n",
    "def plot_feature_importance(X, y, **kwargs):\n",
    "  importances = feature_importance(X, y, **kwargs).sort(columns=\"importance\", ascending=False)\n",
    "\n",
    "  # Plot the feature importances of the forest\n",
    "  plt.figure(figsize=(15, 5), dpi=120)\n",
    "  plt.title(\"Feature importances\")\n",
    "  plt.bar(range(len(importances)), importances['importance'].values, color=\"r\", yerr=importances['std'].values, align=\"center\")\n",
    "  plt.xticks(range(len(importances)), importances.column.values)\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.xlim([-1, len(importances)])\n",
    "  plt.show()\n",
    "\n",
    "def split_dummies(data, train, col):\n",
    "    dummies_train = pd.get_dummies(train[col], prefix=col)\n",
    "    dummies = pd.get_dummies(data[col], prefix=col)\n",
    "    for d_col in dummies_train.columns:\n",
    "        data[d_col] = dummies[d_col].values\n",
    "\n",
    "    print(\"Created dummies for %s: \" % col, dummies_train.columns)\n",
    "    data.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def split_most_common(data, train, col):\n",
    "    mc_mask = np.isclose(data[col], train[col].value_counts().index[0])\n",
    "    data[col + '_mc'] = mc_mask.astype(int)\n",
    "    data[col + '_log'] = normalize(data.loc[~mc_mask, col].map(np.log))\n",
    "    data.set_value(mc_mask, col + '_log', 0)\n",
    "    data[col + '_log'].fillna(0, inplace=True)\n",
    "    \n",
    "    print(\"Created features for %s: \" % col, col + '_mc', col + '_log')\n",
    "    data.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def normalize(data):\n",
    "    return data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "def minmax(data):\n",
    "    xmin =  data.min()\n",
    "    return (data - xmin) / (data.max() - xmin)\n",
    "\n",
    "def target_hist(data, X, y, bins=100, figsize=(15, 5), density=False):\n",
    "    # setting up the axes\n",
    "    fig = plt.figure(figsize=figsize, dpi=120)\n",
    "    targets = np.unique(y)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(targets)))\n",
    "    width = None\n",
    "    _bins = np.linspace(np.min(X), np.max(X), bins, endpoint=True)\n",
    "    s = np.asarray(list(range(len(targets)))) - (len(targets) - 1) * 0.5\n",
    "    \n",
    "    # now plot\n",
    "    for i, t in enumerate(targets):\n",
    "        h, b = np.histogram(X[y == t], bins=_bins, density=density)\n",
    "        center = (b[:-1] + b[1:]) / 2\n",
    "        if width is None:\n",
    "            width = np.abs(center[0] - center[1]) / len(targets) * 0.8\n",
    "        # f = interp1d(center, h, kind='cubic', fill_value=0, bounds_error=False)\n",
    "        # x = np.linspace(np.min(center), np.max(center), num=len(center)*10, endpoint=True)\n",
    "        # plt.plot(x, f(x), label=t)\n",
    "        \n",
    "        offset = s[i] * width\n",
    "        plt.bar(center + offset, h, width=width, align='center', color=colors[i], label=t, alpha=0.75)\n",
    "    \n",
    "    # show\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def feature_hists(data, bins=20, figsize=(15, 5)):\n",
    "  num_data = data[get_numeric_columns(data)]\n",
    "  uniques = num_data.apply(lambda x: len(x.unique()))\n",
    "  bin_options = {col: min(bins, uniques[col]) for col in num_data.columns}\n",
    "\n",
    "  for col in get_categoric_columns(data):\n",
    "    plt.figure(figsize=figsize, dpi=120)\n",
    "    data[col].value_counts().plot(kind='bar')\n",
    "    plt.title(col)\n",
    "    \n",
    "  for col in num_data.columns:\n",
    "    plt.figure(figsize=figsize, dpi=120)\n",
    "    plt.title(col)\n",
    "    data[col].plot(kind='hist', alpha=0.5, bins=bin_options[col])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
